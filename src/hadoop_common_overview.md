### Структура Hadoop Common

**Hadoop Common** складається з кількох ключових модулів, які забезпечують базову функціональність для всієї екосистеми Hadoop. Основні компоненти Hadoop Common:

1. [**Бібліотеки та утиліти**](./common-components/libraries-and-utilities.md): Набір загальних бібліотек Java та утиліт, які використовуються іншими модулями Hadoop. Вони включають інструменти для роботи з файловими системами, серіалізації даних, управління конфігураціями та інші допоміжні функції.

2. [**Файлова система**](./common-components/file-system.md): Інтерфейси та реалізації для роботи з різними файловими системами, включаючи локальні та розподілені файлові системи, такі як HDFS. Це забезпечує абстракцію для роботи з файлами та каталогами незалежно від конкретної файлової системи.

3. [**Інструменти командного рядка**](./common-components/cmd-tools.md): Набір команд для управління та моніторингу кластерів Hadoop. Вони дозволяють виконувати різні операції, такі як копіювання файлів, перегляд вмісту каталогів, управління конфігураціями та інші адміністративні завдання.

4. [**Модуль безпеки**](./common-components/security.md): Засоби для забезпечення автентифікації, авторизації та шифрування даних у кластері Hadoop. Це включає інтеграцію з системами безпеки, такими як Kerberos, для захисту даних та контролю доступу.

5. [**Конфігураційні файли**](./common-components/configs.md): XML-файли, які визначають параметри налаштування для різних компонентів Hadoop. Вони дозволяють централізовано керувати конфігураціями та забезпечують гнучкість у налаштуванні системи.

6. [**Модуль RPC (Remote Procedure Call)**](./common-components/rdc.md): Інфраструктура для віддаленого виклику процедур, яка дозволяє компонентам Hadoop взаємодіяти між собою через мережу. Це забезпечує ефективну та надійну комунікацію між різними частинами системи.
